We’ve trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks. A typical approach to language modeling is to learn the following task: predict the next word, given all of the previous words within some text. Last year, OpenAI’s Generative Pre-trained Transformer (GPT) showed that language models trained on large amounts of data can be fine-tuned to specific tasks to achieve high performance. GPT-2 shows that much larger language models trained on a more diverse dataset derived from the internet begin to learn these NLP tasks without needing task-specific training data, instead learning from examples the system derives from the raw text. These systems also display a substantial qualitative jump in the realism and coherence of generated text.
Used most notably in the Transformer, Attention has enabled deep learning to arguably approach human level performance across modalities with larger models continuing to boost performance. However, the heuristic motivations that produced Attention leave open the question of why it is so good at the same tasks humans are. Insights into why Attention is so effective would not only make it more interpretable but also guiding future improvements. Much work has been done to try and explain the Attention's success, including work showing that Transformers representations map more closely to human brain recordings and inductive biases than other models. Our work goes further in showing the potential relationship between Attention and the brain at the level of the connectome, giving a new and exciting perspective behind Attention's success. This potential relationship to the brain is created by showing mathematically that Attention closely approximates Sparse Distributed Memory (SDM) and is summarized in Figure A. SDM is an associative memory model developed in 1988 to solve the ``Best Match Problem'', where we have a set of memories and want to quickly find the ``best'' match to any given query. In the development of its solution, SDM respected fundamental biological constraints, such as Dale's law that synapses are fixed to be either excitatory or inhibitory and cannot dynamically switch (see Section 1 for an overview of SDM). While being developed independently of brain regions, surprisingly, SDM's biologically plausible solution maps strikingly well onto the cerebellum. At a high level, the relationship between SDM and Attention exists because SDM's update operation approximates the Attention softmax rule. To give intuition for why this is the case, imagine two identical, intersecting spheres with their centers distance $c$ apart. As the distance $c$ between the spheres increases, under certain conditions the volume of their intersection will decay approximately exponentially. Mapping this onto SDM, one sphere defines the neurons that have written a pattern into memory and the other sphere defines the neurons being read from by a query. The volume of the sphere intersection defines the weight or ``attention'' assigned to a pattern. The approximately exponential decay as the distance between the pattern and query spheres increases can be mapped onto the exponential function used in Attention's softmax. Section 2 provides more details and a formal analysis of this approximate exponential and how Attention closely approximates itEstablishing that Attention approximates SDM in theory, we then test it in pre-trained GPT2 Transformer models (Section 3). We use the Query-Key Normalized Transformer variant to directly show that the relationship to SDM holds well. We then use original GPT2 models to help confirm this result and make it more general. Given these results, we put forward the hypothesis that due to the exponential function's rapid ``take-off'' with increasingly large positive inputs, for Attention to work well it may need to exist in a regime where it always closely approximates SDM.Using the SDM framework, we are able to go beyond Attention and interpret the Transformer architecture as a whole, providing its heuristic structure with deeper intuition (Section 4). The fact that SDM maps well onto the whole Transformer suggests that extensions to SDM that already exist could motivate future Transformer improvements.Finally, in related work (Section 5), we explain how SDM is a generalization of Hopfield Networks and, in turn, how our results are also a generalization of work relating Hopfield Networks to Attention. We also discuss Vector Symbolic Architectures that are compatible with SDM and relations to other memory models such as the Neural Turing and Kanerva Machines.To what extent will our most successful deep learning systems converge with the brain? Some convergence seems inevitable, given that we want our models to excel at the same tasks that brains do. However, the brain and machine learning models have different optimizers, objective functions, and constraints. In this light, it is surprising that the connectome of the cerebellum, if it implements Sparse Distributed Memory, closely approximates Attention and many aspects of the whole Transformer architecture. We believe this work provides a small piece of evidence that the brain and the best deep learning models may be more convergent than we might otherwise think.
Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do:  once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, `and what is the use of a book,' thought Alice `without pictures or conversation?'   So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.   There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself, `Oh dear!  Oh dear!  I shall be late!'  (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually TOOK A WATCH OUT OF ITS WAISTCOAT- POCKET, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.   In another moment down went Alice after it, never once considering how in the world she was to get out again.   The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.   Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next.  First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs.  She took down a jar from one of the shelves as she passed; it was labelled `ORANGE MARMALADE', but to her great disappointment it was empty:  she did not like to drop the jar for fear of killing somebody, so managed to put it into one of the cupboards as she fell past it.   `Well!' thought Alice to herself, `after such a fall as this, I shall think nothing of tumbling down stairs!  How brave they'll all think me at home!  Why, I wouldn't say anything about it, even if I fell off the top of the house!' (Which was very likely true.)   Down, down, down.  Would the fall NEVER come to an end!  `I wonder how many miles I've fallen by this time?' she said aloud. `I must be getting somewhere near the centre of the earth.  Let me see:  that would be four thousand miles down, I think--' (for, you see, Alice had learnt several things of this sort in her lessons in the schoolroom, and though this was not a VERY good opportunity for showing off her knowledge, as there was no one to listen to her, still it was good practice to say it over) `--yes, that's about the right distance--but then I wonder what Latitude or Longitude I've got to?'  (Alice had no idea what Latitude was, or Longitude either, but thought they were nice grand words to say.) Presently she began again.  `I wonder if I shall fall right THROUGH the earth!  How funny it'll seem to come out among the people that walk with their heads downward!  The Antipathies, I think--' (she was rather glad there WAS no one listening, this time, as it didn't sound at all the right word) `--but I shall have to ask them what the name of the country is, you know. Please, Ma'am, is this New Zealand or Australia?' (and she tried to curtsey as she spoke--fancy CURTSEYING as you're falling through the air!  Do you think you could manage it?)  `And what an ignorant little girl she'll think me for asking!  No, it'll never do to ask:  perhaps I shall see it written up somewhere.
Previous work showed that the modern Hopfield Network, when made continuous and optimized differently, becomes Attention. This result was one motivation for this work because Hopfield Networks are another form of Associative Memory. However, Hopfield Networks are biologically implausible, lack SDM's geometric framework, are not part of the powerful family of Vector Symbolic Architectures that will be outlined next, and in their original form are less appropriate for handling heteroassociative memory and correlated patterns and the new energy functions of the modern discrete Hopfield network make it a closer approximation to SDM. That SDM is significantly more biologicially plausible than any Hopfield Network variant and has an immediate relationship to Attention that explains the route taken by the Hopfield Network makes this work a generalization of previous insights.Since the publication of SDM there have been a number of advancements not only to SDM specifically, but also through the creation of related associative memory algorithms under the name of ``Vector Symbolic Architectures''. Advancements to SDM specifically include: using integer rather than binary vectors; approaches to handle correlated patterns; hierarchical data storage; and more sophisticated convergence analysis. Vector Symbolic Architectures, most notably Holographic Reduced Representations, have ideas that can be related back to SDM and the Transformer in ways that may be fruitful and will be explored in future work. The use of external memory modules in neural networks has been explored most notably with the Neural Turing Machine (NTM) and its followup, the Differential Neural Computer (DNC). In order to have read and write operations to the external memory be differentiable, they use the softmax function, this combined with their use of cosine similarity between the query and memory locations makes both models closely related to SDM. However, subtle yet arguably important differences exist. In both the NTM and DNC, the memory vectors do not have addresses, only pointers meaning the cosine similarity is between the query and memory contents (analogous to the neuron value vector rather than address). We believe this lack of variable binding between addresses and pointers makes the memory system weaker because one memory cannot point to others. One indication of this weakness is the need for a location based addressing implementation to learn sequence transitions, in addition to the content based addressing outlined. A more recent improvement to the NTM and DNC directly inspired by SDM is the Kanerva Machine. The Kanerva Machine augments a VAE with external memory in the form of SDM and treats it as Bayesian, allowing for Bayesian inference to determine what memories should be stored. However, the Kanerva Machine remains distinct from SDM and Attention because it lacks the softmax function on the cosine similarities between its query and patterns. The model is also restricted to the autoassociative setting with training difficulties noted when the model was used for a heteroassociative reinforcement learning problem. Independent of these discrepancies with SDM, the Kanerva Machine presents a number of interesting ideas that we believe can be related to our work and developed further. The result that Attention approximates SDM should enable more cross pollination of ideas between neuroscience, theoretical models of associative learning, and deep learning. Considering avenues for future deep learning research, SDM's relationship to Vector Symbolic Architectures is particularly compelling because they can apply logical and symbolic operations on memories, making SDM more powerful. Analogizing to computers, SDM is to RAM as VSA is to the CPU. Beyond symbolic approaches, there are extensions to SDM that leverage sparsity and discrete representations that may improve Transformers, similarly to how vector quantization has improved VAEs. SDM and its relation to the brain can inspire new research in not only deep learning research but also neuroscience, because of the empirical success of Transformers and its relation to the cerebellum via SDM. While SDM fits unique features of the cerebellum in striking ways, this connection and its evidence relies on neuroscientific evidence available in 1988. Although we are unaware of any results that disprove this connection and the Marr-Albus-Ito model of the cerebellum that fits with SDM remains popular, in future work we will review modern neuroscience to update the state of SDM's biological plausibility. There are two reasons to be initially excited about this investigation. First, there has been growing evidence that the cerebellum is important for many more cognitive functions than just precise motor control, long believed to be its only role. Second, improvements to our understanding of the computational capabilities of Pyramidal neurons means that they may have parallels to Purkinje cells and that the core algorithmic requirements of SDM could be implemented in additional brain regions such as the hippocampus and cortical columns. These remain early hypotheses but the success of Transformers and their link to SDM should catalyze novel investigations.One limitation of our analysis in this paper is that we cannot use SDM to predict the $\beta$ value that Attention should use.
So, we left off in my senior year of high school. It was 2007. Chris Brown was still a good guy in public eye, Britney Spears and K-Fed had recently split, Michael Jackson was still alive, and my high school boyfriend and I had reached a massive fork in the road. We were accepted into two different universities on opposite sides of the country. He was moving to the other side while I was staying close to home. The good news? We were accepted into our first picks. The bad news? Our relationship was toast. We decided we would enjoy whatever time we had left together, and then go our separate ways. I wasn’t very keen on long distance as I wanted to focus more on my studies than making sure my boyfriend wasn’t cheating on me. Dean agreed. We were exactly on the same page in that regard. When time came around for us to say our goodbyes, I made sure to let him know he was as close to perfect as I had ever met. Not only was he patient, supportive and trusting, he was all of those things and so effortlessly understanding at the same time. Of course we had our moments, but it was all worth it. I can safely say I was the lucky one in our relationship. I’ve heard so many horror stories of crazy, manipulative boyfriends but Dean wasn’t like that at all. He was good. So a couple of months into university, when I heard he was dating another girl at his school, I wasn’t at all surprised. Dean was a charmer. Of course there was another girl lol. She was cute. I had only a few pictures to go off of on social media, but yeah. She very cute, very bubbly looking, and exactly the type of girl I had always imagined for Dean. In other words I was happy for them. I mean, don’t get me wrong, I had my jealous moments … but I was mostly happy for them. The fact that I was suddenly thrust into university and forced to make new friends at school, helped to distract me from those tiny, unwelcome sparks of jealousy. I eventually wasn’t jealous anymore, and I eventually met this girl when Dean brought her home for Christmas. Our tight knit group of friends from high school had all met up at one of our houses the day before Christmas Eve. Dean politely introduced his girlfriend to me, and we got on really well. She was a nice person. Still is, as a matter of fact. (They’re married now and pregnant with their first child. Crazy, huh?) The next night, when I was with my family and a bunch of our relatives at my parents’ Christmas Eve party, I realized how much I missed home. School wasn’t too far away. Maybe two hours on a particularly busy day. But during those first few months, I tried my best not to go home and fall into old, comfortable habits. I wanted to grow up and separate myself from my family a little bit. That said, I absolutely cried when I saw them again. Such a cornball lol. My brother made fun of me for years over that. In fact he still does. He’s kind of an asshole like that, but in a good way. Suffice to say I stuffed my face and immediately fell into food coma. It was close to midnight, I would say. I dragged myself into my old bedroom, where everything had been left the same since the last time I was there. The murmur of music and laughter down below kept me somewhat alert of my surroundings. I knew it wouldn’t be long before my mother or father came knocking to see if I wanted some extra dessert or something, so I told myself I would rest for only a few minutes. The second I flopped down in bed and sunk into the comfy mattress I been deprived of at school (seriously, the mattress in my dorm was like a slab of concrete) I felt my pocket vibrate. I groaned into my pillow, lazily retrieving the flip phone from my pocket to find a text message. From: Nicky Merry Christmas However tired I felt, I smiled when I saw the message. There were times when we would go months without speaking, and times when we would speak nearly everyday. No matter how long we had gone without talking, we’d always pick up exactly where we left off as though no time had passed. He was going to school in the same city as me. But, for some reason, we had yet to hang out. We tried a few times in the beginning, organizing lunch or coffee somewhere in the city, but the plan would always change. We’d run into scheduling problems or papers that were due. For the most part, he was the one to cancel on me. I figured he wanted to focus on school and his new friends
So this takes place back in high school, long time ago. I was in class and I wasn't feeling too good that day. I wish I was kidding, I really do but I'm not. I have a bowel disorder with the intestines, not gonna get into that. But I was in class, didn't shit for about a week and a half if I remember correctly. I had to take a shit. Really fucking bad. My stomach was aching so bad. I would NEVER shit at someones house, or at school. Except for that day. I thought to myself "Fuck it, I'll take a shit in the bathroom. It's gonna be the biggest shit anyone has seen, but most likely they'll have no idea who it was. So I asked the teacher to use the bathroom, took a hall pass and was on my way. I get into that bathroom, nobody was in it a the time (thank god) and immediately run to the stall, close it, lock it, drop my pants and sit on the toilet. About 5-7 seconds later, there it goes. It was thirty seconds of pushing, one came out, and then another one. I could smell the defecation as soon as it hit the water in the toilet, and it was awful. This wasn't going to flush, there's no way. But then a student walked in. My heart sank from my chest all the way to my ballsack probably.I was genuinely terrified. The student walked right in. He said "oh god what the fuck is that" he took a piss, and left as quick as possible. I sat on the toilet for the next few minutes. My stomach was aching, but I also felt relief. No, I didn't flush it. Didn't even try, I knew it would clog. I didn't want anyone to suspect it was me in even the slightest. I eventually left the bathroom, and walked back to class. I was in there for a bit, so the teacher said I was late on the way back. With some quick thinking I tell her "I'm sorry miss, Won't lie to you, my friends walked in and we got caught up in silly conversation" She replies to me "Well, at least you're telling the truth, I appreciate that. But you're behind on work" I went back to my desk, finished my stuff. At the end of the day, nobody had an idea who it was that took this giant shit in the toilet and left it there. I went home that day, did things a teenager does at home after school, usual shenanigans. Took a piss, video games, half-assed my homework, busted a nut all that stuff. Went back to school the next day. My first thought is "I'm going to that bathroom to see if that shit is still there, if the janitors did anything about it. So I walked in, and that entire stall had a sign that read "out of order" the toilet was closed up and everything. I was dying of laughter, it was absolutely hilarious. I tried so hard to contain my laughter which I did. I thought it was fucking hysterical coming back the next day and seeing that same toilet out of order the next day. It stayed like that for maybe another month. And to this very day, nobody had any idea who did it. This was definitely one of my weirdest, but also funniest HS experiences I've ever had.
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].
This chapter describes one basic model of associative memory, called the sparse distributed memory, and relates it to other models and circuits: to ordinary computer memory, to correlation-matrix memories, to feed-forward artificial neural nets, to neural circuits in the brain, and to associative-memory models of the cerebellum. Presenting the various designs within one framework will hopefully help the reader see the similarities and the differences in designs that are often described in different ways. 3.1.1. Sparse Distributed Memory as a Model of Human Long-Term Memory Sparse Distributed Memory (SDM) was developed as a mathematical model of human long-term memory (Kanerva 1988). The pursuit of a simple idea led to the discovery of the model, namely, that the distances between concepts in our minds correspond to the distances between points of a high-dimensional space. In what follows, ‘high-dimensional’ means that the number of dimensions is at least in the hundreds, although smaller numbers of dimensions are often found in examples. If a concept, or a percept, or a moment of experience, or a piece of information in memory—a point of interest—is represented by a high-dimensional (or “long”) vector, the representation need not be exact. This follows from the distribution of points of a high-dimensional space: Any point of the space that might be a point of interest is relatively far from most of the space and from other points of interest. Therefore, a point of interest can be represented with considerable slop before it is confused with other points of interest. In this sense, long vectors are fault-tolerant or robust, and a device based on them can take advantage of the robustness. This corresponds beautifully to how humans and animals with advanced sensory systems and brains work. The signals received by us at two different times are hardly ever identical, and yet we can identify the source of the signal as a specific individual, object, place, scene, thing. The representations used by the brain must allow for such identification, in fact, they must make the identification nearly automatic, and high-dimensional vectors as internal representations of things do that. Another property of high-dimensional spaces also has to do with the distances between points. If we take two points (of interest) at random, they are relatively far from each other, on the average: they are uncorrelated. However, there are many points between the two that are close to both, in the sense that the amount of space around an intermediate point—in a hypersphere—that contains both of the two original points is very small. This corresponds to the relative ease with which we can find a concept that links two unrelated concepts. Strictly speaking, a mathematical space need not be a high-dimensional vector space to have the desired properties; it needs to be a huge space, with an appropriate similarity measure for pairs of points, but the measure need not define a metric on the space. The important properties of high-dimensional spaces are evident even with the simplest of such spaces—that is, when the dimensions are binary. Therefore, the sparse distributed memory model was developed using long (i.e., highdimensional) binary vectors or words. The memory is addressed by such words, and such words are stored and retrieved as data. The following two examples demonstrate the memory’s robustness in dealing with approximate data. The memory works with 256-bit words: it is addressed by them, and it stores and retrieves them. On top of Figure 3.1 are nine similar (20% noisy) 256-bit words. To help us compare long words, their 256 bits are laid on a 16-by-16 grid, with 1s shown in black. The noise-free prototype word was designed in the shape of a circle within the grid. (This example is confusing in that it might be taken to imply that humans recognize circles based on stored retinal images of circles. No such claim is intended.) The nine noisy words were stored in a sparse distributed memory autoassociatively, meaning that each word was stored with itself as the address. When a tenth noisy word (bottom left), different from the nine, was used as the address, a relatively noise-free 11th word was retrieved (bottom middle), and with that as the address, a nearly noise-free 12th word was retrieved (bottom right), which in turn retrieved itself. This example demonstrates the memory’s tendency to construct a prototype from noisy data.
Biological neurons are cells that process signals in animals and humans, allowing them to respond rapidly to the environment. To achieve speed, neurons use electrochemical mechanisms to generate a signal (a voltage level or electrical pulses) and to transmit it to nearby and distant sites. Biological neurons come in many varieties. The peripheral neurons couple the organism to the world. They include the sensory neurons that convert an external stimulus into an electrical signal, the motor neurons whose electrical pulses cause muscle fibers to contract, and other effector neurons that regulate the secretion of glands. However, most neurons in highly evolved animals are interneurons that connect directly to other neurons rather than to sensors or to effectors. Interneurons also come in many varieties and they are organized into a multitude of neural circuits. A typical interneuron has a cell body and two kinds of arborizations: a dendrite tree that receives signals from other neurons, and an axon tree that transmits the neuron’s signal to other neurons. Transmission-contact points between neurons are called synapses. They are either excitatory (positive synaptic weight) or inhibitory (negative synaptic weight) according to whether a signal received through the synapse facilitates or hinders the activation of the receiving neuron. The axon of one neuron can make synaptic contact with the dendrites and cell bodies of many other neurons. Thus, a neuron receives multiple inputs, it integrates them, and it transmits the result to other neurons. Artificial neural networks are networks of simple, interconnected processing units, called (artificial) neurons. The most common artificial neuron in the literature has multiple (N) inputs and one output and is defined by a set of input coefficients—a vector of N reals, standing for the synaptic weights—and a nonlinear scalar activation function. The value of this function is the neuron’s output, and it serves as input to other neurons. A linear threshold function is an example of an artificial neuron, and the simplest kind—one with binary inputs and output—is used in the sparse distributed memory. It may seem strange to model brain activity with binary neurons when real neurons are very complex in comparison. However, the brain is organized in large circuits of neurons working in parallel, and the mathematical study of neural nets is aimed more at understanding the behavior of circuits than of individual neurons. An important fact—perhaps the most important—is that the states of a large circuit can be mapped onto the points of a high-dimensional space, so that although a binary neuron is a grossly simplified model of a biological neuron, a large circuit of binary neurons, by virtue of its high dimension, can be a useful model of a circuit of biological neurons. The sparse distributed memory’s connection to biology is made in the standard way. Each row through A, d, y, and C in Figure 3.9—each hidden unit—is an artificial neuron that represents a biological neuron. Vector x represents the N signals coming to these neurons as inputs from N other neurons (along their axons), vector Am represents the weights of the synapses through which the input signals enter the mth neuron (at its dendrites), dm represents the integration of the input signals by the mth neuron, and ym represents the output signal, which is passed along the neuron’s axon to U other neurons through synapses with strengths Cm. We will call these (the hidden units) the address-decoder neurons because they are like the address-decoder circuit of a random-access memory: they select locations for reading and writing. The address that the mth address-decoder neuron decodes is given by the input coefficients Am; location Am is activated by inputs x that equal or are sufficiently similar to Am. How similar, depends on the radius of activation H. It is interesting that a linear threshold function with N inputs, which is perhaps the oldest mathematical model of a neuron, is ideal for address decoding in the sparse distributed memory, and that a proper choice of a single parameter, the threshold, makes it into an address decoder for a location of an ordinary randomaccess memory.